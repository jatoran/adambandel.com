Date,Category,Title,Description
1673,,Leibniz Chain Rule,The backpropagation algorithm is an efficient application of the Leibniz chain rule (1673)[24] to networks of differentiable nodes.
1700s,,,"Linear Regression - Gauss (1795), Legendre (1795), Francis Galton (1922-1911)"
1925,RNN,,"Wilhelm Lenz and Ernst Ising created and analyzed the Ising model (1925)[33] which is essentially a non-learning artificial recurrent neural network (RNN) consisting of neuron-like threshold elements.  In 1972, Shun'ichi Amari made this architecture adaptive.[34][9] His learning RNN was popularised by John Hopfield in 1982."
1940s,,,"In the early 1940s, D. O. Hebb[14] created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. "
1943,,,"McCulloch and Pitts introduce the concept of a mathematical model of a neuron, setting the stage for artificial neural network research."
1960s & 1970s,CNN,,"The Hubel and Wiesel experiments greatly expanded the scientific knowledge of sensory processing. In one experiment, done in 1959, they inserted a microelectrode into the primary visual cortex of an anesthetized cat. They then projected patterns of light and dark on a screen in front of the cat. They found that some neurons fired rapidly when presented with lines at one angle, while others responded best to another angle."
1948,,,Researchers started applying Hebbian learning ideas to computational models in 1948 with Turing's B-type machines.
1950,,,"Alan Turing publishes ""Computing Machinery and Intelligence,"" introducing the Turing Test as a way to gauge machine intelligence."
1954,,,"Farley and Clark[15] (1954) first used computational machines, then called ""calculators"", to simulate a Hebbian network."
1956,,,"The Dartmouth Summer Research Project on Artificial Intelligence, widely considered the founding event of the AI field. John McCarthy coins the term 'artificial intelligence.'"
1957,,,"Frank Rosenblatt develops the Perceptron, a simple single-layer neural network and a precursor to modern deep learning."
1960s,,,"Early AI research experiences its first winter; funding decreases and progress slows due to limitations in computing power and unrealistic expectations.  Some say that research stagnated following Minsky and Papert (1969),[18] who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to process useful neural networks. However, by the time this book came out, methods for training multilayer perceptrons (MLPs) by deep learning were already known."
1965,,,"The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965, as the Group Method of Data Handling.[19][20][21] This method employs incremental layer by layer training based on regression analysis, where useless units in hidden layers are pruned with the help of a validation set."
1967,,,"The first deep learning MLP trained by stochastic gradient descent[22] was published in 1967 by Shun'ichi Amari.[23][9] In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned useful internal representations to classify non-linearily separable pattern classes."
1969,CNN,,"In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function.[43][9] The rectifier has become the most popular activation function for CNNs and deep neural networks in general"
1972,RNN,,"In 1972, Shun'ichi Amari made non-learning artificial recurrent neural network architecture adaptive.[34][9] His learning RNN was popularised by John Hopfield in 1982."
1980s,Hardware,,"The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI), combining millions or billions of MOS transistors onto a single chip in the form of complementary MOS (CMOS) technology, enabled the development of practical artificial neural networks in the 1980s."
1980,CNN,,"The origin of the CNN architecture is the ""neocognitron""[40] introduced by Kunihiko Fukushima in 1980.  It was inspired by work of Hubel and Wiesel in the 1950s and 1960s which showed that cat visual cortices contain neurons that individually respond to small regions of the visual field. The neocognitron introduced the two basic types of layers in CNNs: convolutional layers, and downsampling layers."
1982,,,"In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard."
1982,,,Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982.[36][37] SOMs are neurophysiologically inspired[38] artificial neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning.
1987,CNN,,"The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel and was one of the first CNNs, as it achieved shift invariance.  It did so by utilizing weight sharing in combination with backpropagation training.[46] Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights instead of a local one."
1988,CNN,,"In 1988, Wei Zhang et al. applied backpropagation to a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition."
1989,CNN,,"In 1989, Yann LeCun et al. trained a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days.[49] Learning was fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types."
1990,CNN,,"In 1990 Yamaguchi et al. introduced max-pooling, a fixed filtering operation that calculates and propagates the maximum value of a given region. They combined TDNNs with max-pooling in order to realize a speaker independent isolated word recognition system."
1991,GAN,,"In 1991, Juergen Schmidhuber published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss.[68][69][70] The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity."" Earlier adversarial machine learning systems ""neither involved unsupervised neural networks nor were about modeling data nor used gradient descent."
1991,CNN,,"Wei Zhang, et al. modified Yann LeCun's 1989 CNN model by removing the last fully connected layer and applied it for medical image object segmentation in 1991"
1991,LSTM,,"Sepp Hochreiter analyzed the vanishing gradient problem and developed principles of the method in his German diploma thesis,[2] which was called ""one of the most important documents in the history of machine learning"" by his supervisor Juergen Schmidhuber."
1992,GAN,,"In 1992, Schmidhuber published another type of gradient-based adversarial neural networks where the goal of the zero-sum game is to create disentangled representations of input patterns. This was called predictability minimization."
1992,TRNS,,"in 1992, Juergen Schmidhuber published the Transformer with ""linearized self-attention"" (save for a normalization operator),[80] which is also called the ""linear Transformer.""[81][82][9] He advertised it as an ""alternative to RNNs""[80] that can learn ""internal spotlights of attention,""[83] and experimentally applied it to problems of variable binding."
1992,DL,,"Juergen Schmidhuber (1992) proposed a self-supervised hierarchy of RNNs pre-trained one level at a time by self-supervised learning.[86] This ""neural history compressor"" uses predictive coding to learn internal representations at multiple self-organizing time scales."
1993,RNN,,"In 1993, a neural history compressor system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time.[15]"
1995,LSTM,,"""Long Short-Term Memory (LSTM)"" is published in a technical report by Sepp Hochreiter and Jürgen Schmidhuber."
1997,LSTM,,"The main LSTM paper is published in the journal Neural Computation.[1] By introducing Constant Error Carousel (CEC) units, LSTM deals with the vanishing gradient problem. The initial version of LSTM block included cells, input and output gates."
1997,,,"IBM's Deep Blue defeats chess world champion Garry Kasparov, marking a symbolic victory for AI."
1998,CNN,,"LeNet-5, a 7-level CNN by Yann LeCun et al. in 1998,[58] that classifies digits, was applied by several banks to recognize hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images. The ability to process higher-resolution images requires larger and more layers of CNNs, so this technique is constrained by the availability of computing resources."
2006,DL,,Geoffrey Hinton et al. (2006) proposed learning a high-level internal representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine[88] to model each layer.
2010,CNN,,"In 2010, Backpropagation training through max-pooling was accelerated by GPUs and shown to perform better than other pooling variants."
2011,CNN,,"In 2011, a deep GPU-based CNN called ""DanNet"" by Dan Ciresan, Ueli Meier, and Juergen Schmidhuber achieved human-competitive performance for the first time in computer vision contests."
2012,CNN,,"AlexNet (a GPU-based CNN by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton) wins the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), demonstrating the power of deep convolutional neural networks (CNNs) for image classification."
2014,GAN,,"In 2014, this adversarial principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al.[71] Here the environmental reaction is 1 or 0 depending on whether the first network's output is in a given set. This can be used to create realistic deepfakes."
2015,LSTM,,"In 2015, Rupesh Kumar Srivastava, Klaus Greff, and Schmidhuber used LSTM principles to create the Highway network, a feedforward neural network with hundreds of layers, much deeper than previous networks."
2015,CNN,,"A very deep CNN with over 100 layers by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun of Microsoft won the ImageNet 2015 contest"
2015,,,"AlphaGo, developed by DeepMind, defeats a top professional Go player, a game long considered too complex for computers to master."
2017,,,"The Transformer architecture is introduced, revolutionizing natural language processing (NLP) and enabling more advanced language models."
2018,GAN,,"Nvidia's StyleGAN (2018)[75] is based on the Progressive GAN by Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.[76] Here the GAN generator is grown from small to large scale in a pyramidal fashion. StyleGANs improve consistency between fine and coarse details in the generator network."
2018 June,TRNS,,"OpenAI releases GPT-1, a language model showcasing the power of the Transformer."
2018 October,TRNS,,"Google AI releases BERT, another Transformer-based model that achieves state-of-the-art results on NLP tasks."
2019 February,TRNS,,OpenAI releases the larger and more powerful GPT-2.
2020 June,TRNS,,"OpenAI releases the even larger and more capable GPT-3, raising concerns and excitement about its human-like text generation."
2022 March,,,"OpenAI releases DALL-E, demonstrating image generation from text descriptions."
2022 July,,,"Midjourney (beta) popularizes artistic image generation, further fueling public interest in AI."
2022 November,TRNS,,"OpenAI releases ChatGPT, a powerful chatbot based on GPT-3.5, impressing users with its conversational ability."
2023 March,TRNS,,"OpenAI releases the anticipated GPT-4, pushing the boundaries of language models even further."