---
title: Transformer Learning
summary: End-to-end, manifest-driven data curation, tokenizer specialization, and synthetic DPO generation for a 1B-parameter English-only Transformer.
tags: [LLM, Data Curation, Tokenizer, DPO, DuckDB, Streamlit]
status: Active
date: 2025-09-29
cover_image: /project-corpus/placeholder.svg
alt: Diagram-style placeholder representing a manifest-driven curation pipeline
permalink: /project-corpus/projects/transformer-learning/
---
<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>{{ page.title }} — Project</title>
    <meta name="description" content="{{ page.summary }}" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet" />
    <link rel="stylesheet" href="/project-corpus/assets-styles.css" />
  </head>
  <body>
    <header class="site-header">
      <div class="container">
        <a href="/project-corpus/" style="text-decoration:none;color:var(--muted)">← Back to all projects</a>
        <h1 class="project-title">{{ page.title }}</h1>
        <p class="project-subtitle">{{ page.summary }}</p>
      </div>
    </header>
    <main class="container project-main">
      <section class="project-content">
        <div class="hero">
          <img src="{{ page.cover_image }}" alt="{{ page.alt }}" />
        </div>

        <h2>Problem</h2>
        <p>
          Building a small but capable, <strong>1B-parameter English-only Transformer</strong> end-to-end requires
          more than model code: the heavy lift is <em>data</em>. Raw web corpora need robust curation, safety
          filters, and deduplication; instruction-tuning and preference data must be cleanly integrated; and the
          tokenizer should reflect the project’s English–code–math focus. The goal is a practical, reproducible
          pipeline that turns messy sources into <strong>training-ready artifacts</strong> while remaining affordable and
          Windows-friendly for development.
        </p>

        <h2>Approach</h2>
        <p>
          The repository centers on a <strong>manifest-driven curation DAG</strong> powered by DuckDB and YAML configs.
          It standardizes ingestion, cleaning, annotation, and filtering across diverse datasets; specializes a
          SentencePiece tokenizer via <strong>pruning + byte fallback</strong>; and programmatically generates <strong>synthetic
          DPO preference pairs</strong> with an async, resumable workflow. A Streamlit dashboard provides instant
          inspection of pipeline state and quality metrics. Training is planned to run on a budget-friendly
          Linux GPU pod while authoring and orchestration remain smooth on Windows.
        </p>

        <h3>Architecture</h3>
        <ul>
          <li>Manifest‑Driven Curation DAG — YAML‑defined stages (s00–s20) with <code>run_pipeline.py</code> orchestrator.</li>
          <li>Immutable Ledger — Central <code>data/manifest.db</code> in DuckDB tracks step outputs per dataset.</li>
          <li>Type‑Aware Execution — Steps auto‑apply based on <em>category</em>, <em>content_type</em>, and <em>schema_type</em>.</li>
          <li>Quality Signals — Precomputed token/char counts and heuristics enable fast SQL‑only filtering.</li>
          <li>Tokenizer Subsystem — Train candidate BPE, then prune by whitelist + frequency to specialize.</li>
          <li>Synthetic DPO — Async, config‑driven generation with resumable state for preference pairs.</li>
          <li>Dashboard — Streamlit app to explore pipeline funnel, dataset profiles, SQL, and logs.</li>
        </ul>

        <h3>Stack</h3>
        <ul>
          <li>Python 3.11, DuckDB, Pandas, PyArrow — Data plumbing and SQL‑centric curation.</li>
          <li>SentencePiece, tiktoken — Tokenizer training, auditing, and evaluation.</li>
          <li>datasketch, xxhash — Near‑duplication (MinHash LSH) and hashing.</li>
          <li>ftfy, BeautifulSoup, markdownify — Text normalization and HTML→Markdown cleaning.</li>
          <li>Presidio Analyzer — PII detection/redaction in early shard processing.</li>
          <li>Transformers, Torch — Perplexity tagging, future training integration.</li>
          <li>Streamlit — Data curation dashboard with read‑only DB attach.</li>
        </ul>

        <h3>Key Modules</h3>
        <ul>
          <li><code>scripts/curation_pipeline/run_pipeline.py</code> — Orchestrates DAG using <code>curation_globals.yml</code>.</li>
          <li><code>configs/datasets/curation_configs/</code> — Step configs (validation→final filters) with applicability rules.</li>
          <li><code>scripts/dataset_utils/</code> — Config loading, logging, token/PII utilities, schema analysis.</li>
          <li><code>scripts/tools/</code> — Reporting, shard inspection, global dedup, validation helpers.</li>
          <li><code>scripts/tokenizer/</code> — Sample creation, training, evaluation, pruning/re‑packaging.</li>
          <li><code>scripts/dpo_generation/</code> — Async client, state manager, and main generator.</li>
          <li><code>scripts/curation_dashboard/</code> — Streamlit views: funnel, profiles, SQL explorer, inspector.</li>
        </ul>

        <h3>Pipeline Highlights</h3>
        <ul>
          <li>Exact + Near Dedup — SHA256 + MinHash/LSH across partitions to remove duplicates.</li>
          <li>Language & Toxicity Filters — FastText‑style lang detection; toxicity scoring for safety.</li>
          <li>Perplexity Tagging — Multi‑model PPL to suppress gibberish and up‑weight complexity.</li>
          <li>Benchmark Decontamination — Bloom filter‑based overlap checks for fair evaluations.</li>
          <li>Axis & Control Tags — Annotates SFT/DPO with behavior axes and formatting flags.</li>
          <li>Smoke vs Production — <code>PIPELINE_ENV</code> switches to a test sandbox with tiny artifacts.</li>
        </ul>

        <h3>Challenges</h3>
        <ul>
          <li>Heterogeneous Schemas — Solved via a canonical mapping during Parquet conversion.</li>
          <li>Performance vs. Cost — SQL‑only filtering on precomputed signals keeps GPU time low.</li>
          <li>Determinism — Immutable, step‑named tables and logged metrics enable auditability.</li>
          <li>Tokenizer Drift — Pruning by whitelist + frequency preserves merges while shrinking vocab.</li>
          <li>Async DPO Robustness — Resume‑safe state file and rate‑limit aware client.</li>
        </ul>

        <h2>Outcomes</h2>
        <p>
          A cohesive, end‑to‑end <strong>data engine</strong> that prepares high‑quality, analysis‑ready corpora; a
          <strong>specialized tokenizer</strong> aligned to English/code/math; and a <strong>synthetic DPO</strong>
          pipeline for alignment experiments. Together, these systems enable training and post‑training studies
          for a compact Transformer on modest hardware while preserving repeatability and cost control.
        </p>

        <section class="updates">
          <h2>Updates</h2>
          {% assign updates_base = page.url | append: 'updates/' %}
          {% assign updates = site.pages | where_exp: 'u', "u.url contains updates_base" | sort: 'date' | reverse %}
          {% if updates.size > 0 %}
            {% for u in updates limit: 5 %}
              <article class="update">
                <h3><a href="{{ u.url }}">{{ u.title }}</a></h3>
                {% if u.date %}<time datetime="{{ u.date | date_to_xmlschema }}">{{ u.date | date: '%b %d, %Y' }}</time>{% endif %}
                {% if u.excerpt %}<p>{{ u.excerpt }}</p>{% endif %}
              </article>
            {% endfor %}
          {% else %}
            <p>No updates yet.</p>
          {% endif %}
        </section>
      </section>

      <aside class="project-aside">
        <dl class="facts">
          <dt>Status</dt>
          <dd><span class="badge status {{ page.status | downcase }}">{{ page.status }}</span></dd>
          <dt>Tags</dt>
          <dd>{{ page.tags | join: ', ' }}</dd>
          <dt>Started</dt>
          <dd>{{ page.date | date: '%b %Y' }}</dd>
          <dt>Links</dt>
          <dd>
            <ul>
              <li><a href="#">Live demo</a></li>
              <li><a href="#">GitHub</a></li>
            </ul>
          </dd>
        </dl>
      </aside>
    </main>
    <footer class="site-footer">
      <div class="container">
        <small>© {{ "now" | date: "%Y" }} Adam Bandel</small>
      </div>
    </footer>
  </body>
  </html>

